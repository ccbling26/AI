# NLP 的里程碑进展

| 年份 | 2013     | 2014  | 2015              | 2016           | 2017        | 2018                          | 2019                                         | 2020                         |
| ---- | -------- | ----- | ----------------- | -------------- | ----------- | ----------------------------- | -------------------------------------------- | ---------------------------- |
| 技术 | Word2Vec | Glove | LSTM<br>Attention | Self-Attention | Transformer | GPT<br/>ELMo<br/>BERT<br/>GNN | XLNet<br/>BoBERTa<br/>GPT-2<br/>ERNIE<br/>T5 | GPT-3<br/>ELECTRA<br/>ALBERT |



# Onehot（独热）编码

把单词用向量表示，是把深度神经网络语言模型引入自然语言处理领域的一个核心技术

无法解决词之间的相似性问题，对于 Onehot 编码表示的向量，如果采用**余弦相似度**计算向量间的相似度，会发现任意两个向量都不相关（相似度结果都为 0）



# Word Embedding 词向量

假设词典大小为 V，那么可以初始化一个 $V\times m$ 的矩阵 Q，通过训练，Q 中的内容会被正确赋值，最后得到所有单词的词向量（矩阵 Q 每一行对应一个单词）

获取词向量的语言模型有：

- Word2Vec，其网络结构和神经网络语言模型（NNLM）基本类似，训练方法有两种

  > NNLM 是通过词的上文预测这个词

  - CBOW：从一个句子中抠掉一个词，用这个词的上下文预测被抠掉的词
  - Skip-Gram：与 CBOW 相反，输入某个单词，预测其上下文

- Glove



# 预训练

需要预训练的原因

- 训练数据小，不足以训练复杂网络
- 加快训练速度
- 参数初始化，先找到好的初始点，有利于优化

预训练的思想

- 任务 B 进行预先训练得到模型 B（模型 B 的参数一开始是随机初始化的）
- 利用模型 B 的参数对模型 A 进行初始化（任务 A 对应的模型 A 的参数不再是随机初始化的）
- 通过任务 A 的数据对模型 A 进行训练



## 图像领域的预训练

CNN 卷积神经网络，一般用于图片分类任务，由多个层级结构组成，不同层学到的图像特征也不同

- 越浅的层学到的特征越通用（横竖撇捺）
- 越深的层学到的特征和具体任务的关联性越强（人脸-人脸轮廓、汽车-汽车轮廓）

具体做法

- 通过 ImageNet 数据集训练出一个模型 A
- 由于上面提到 CNN 的浅层学到的特征通用性特别强，可以对模型 A 做出一部分改进得到模型 B（两种方法）
  - Frozen：浅层参数使用模型 A 的参数，高层参数随机初始化，浅层参数一直不变，然后开始训练参数
  - Fine-Tuning 微调：浅层参数使用模型 A 的参数，高层参数随机初始化，然后开始训练参数，但是在这里浅层参数会随着任务的训练不断发生变化